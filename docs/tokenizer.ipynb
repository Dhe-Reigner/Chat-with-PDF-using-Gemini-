{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 9587\n",
      "Global Rewilding Initiative: Rewilding People & Nature\n",
      "Learn how one person, even of limited means,\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "pdf_path = \"sustainable_development.pdf\"\n",
    "\n",
    "# Open the PDF file in binary mode\n",
    "with open(pdf_path, \"rb\") as pdf_file:  \n",
    "    reader = PyPDF2.PdfReader(pdf_file)\n",
    "    \n",
    "    # Extract text from all pages\n",
    "    raw_text = \"\"\n",
    "    for page in reader.pages:\n",
    "        raw_text += page.extract_text()\n",
    "    \n",
    "    print(\"Total number of characters:\", len(raw_text))\n",
    "    print(raw_text[:99])  # Print the first 99 characters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to tokenize 9587 characters into individual words and special characters that we can then turn into embeddings for LLM training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An Example of Text Splitting ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re # This library(regular expression) helps split the text to help obtain a list of tokens\n",
    "\n",
    "text = \"Hello, world. This is a test.\"\n",
    "result = re.split(r'(\\s)', text) # (\\s) splits when whitespaces are encountered\n",
    "\n",
    "print(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['Hello,', ' ', 'world.', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test.']\n",
    "This is the results printed out, which is a list of individual words,whitespaces and punctuation characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify the regular expression splits on whitespaces(\\s), commas, and periods([,.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the whitespaces ###  This reduces memory and computation requirements. However keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text(eg Python code which is sensitive to indentation and spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "# Splitting all other characters like commas that can be present in the text\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!#$\"()<>\\']|--|\\s)', text)      #### This line and\n",
    "result = [item.strip() for item in result if item.strip()]  #### This line involves our tokenization scheme\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "# Strip whitespaces from each item and then filter out any empty strings.\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets now tokenize our text pdf that we are using above \"sustainable_development.pdf\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Global', 'Rewilding', 'Initiative', ':', 'Rewilding', 'People', '&', 'Nature', 'Learn', 'how', 'one', 'person', ',', 'even', 'of', 'limited', 'means', ',', 'can', 'plant', 'a', 'forest', '.', 'Home', '›', 'Articles', '›', '17', 'Sustainable', 'Development']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!#$\"()<>\\']|--|\\s)', raw_text)  # preprocessed is just a variable\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1615\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creating a list of all unique tokens and sorting them out alphabetically to determine the vocabulary size ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    " \n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assign integer values(from zero)to all tokens(which include punctuation marks, numbers and alphabetical letters,in alphabetical order)\n",
    "vocab = {token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('$', 0)\n",
      "('&', 1)\n",
      "('(', 2)\n",
      "(')', 3)\n",
      "(',', 4)\n",
      "('.', 5)\n",
      "('//glorew', 6)\n",
      "('1', 7)\n",
      "('1/4GOAL', 8)\n",
      "('10', 9)\n",
      "('11', 10)\n",
      "('11/24/24', 11)\n",
      "('12', 12)\n",
      "('13', 13)\n",
      "('14', 14)\n",
      "('15', 15)\n",
      "('16', 16)\n",
      "('17', 17)\n",
      "('2', 18)\n",
      "('2/4table', 19)\n",
      "('2015', 20)\n",
      "('2020', 21)\n",
      "('2024', 22)\n",
      "('2030', 23)\n",
      "('25', 24)\n",
      "('3', 25)\n",
      "('3/4About', 26)\n",
      "('35', 27)\n",
      "('4', 28)\n",
      "('4/4', 29)\n",
      "('5', 30)\n",
      "('500', 31)\n",
      "('6', 32)\n",
      "('7', 33)\n",
      "('8', 34)\n",
      "('9', 35)\n",
      "(':', 36)\n",
      "(';', 37)\n",
      "('?', 38)\n",
      "('A', 39)\n",
      "('AM', 40)\n",
      "('About', 41)\n",
      "('Achieve', 42)\n",
      "('Action', 43)\n",
      "('Affordable', 44)\n",
      "('Agenda', 45)\n",
      "('Agriculture', 46)\n",
      "('Always', 47)\n",
      "('An', 48)\n",
      "('Articles', 49)\n",
      "('ArticlesArticles', 50)\n"
     ]
    }
   ],
   "source": [
    "# printing first 51 entries for illustration purposes\n",
    "for i , item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As seen from the output above, the dictionary contains individual tokens with unique integer labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets now implement a complete tokenizer class in python\n",
    "###  This class will have both an encode and decode method\n",
    "#### Encode method takes text as input and gives out token ids as output\n",
    "#### Decode method takes token ids as input and gives out text as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer class\n",
    "\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()} # s token i token id\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!#$\"()<>\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenize a passage  from the pdf uploaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[137, 556, 17, 355, 4, 226, 270, 556, 87, 51, 452, 556, 201, 132, 246, 45, 339, 189, 70, 4]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"Of the 17 goals, adopted by the\n",
    "General Assembly of the United\n",
    "Nations as Agenda for Sustainable\n",
    "Development,\"\"\"\n",
    "ids = tokenizer.encode(text) # Encode turns text to ids\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Of the 17 goals, adopted by the General Assembly of the United Nations as Agenda for Sustainable Development,'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids) # Decode turns the ids into text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We implemented a tokenizer capable of tokenizing and de-tokenizing text based on a snippet from the training set.\n",
    "\n",
    "\n",
    "Let's now apply it to a new text sample that is not contained in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      9\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!#$\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()<>\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m     10\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     12\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hello was not used in our tutorial hence its un unknown word hence there is an exception...\n",
    "\n",
    "##### Having large and diverse datasets helps cater for new words so there wont be exceptions of unknown words\n",
    "\n",
    "\n",
    " ##### SPECIAL CONTEXT TOKENS #####\n",
    "This deals with the unknown words for a training model hence not to show an error message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will modify the vocabulary and tokenizer we implemented in the previous section, SimpleTokenizerV2, to support 2 new tokens: <|unk|> and <|endoftext|> and they are assigned token ids respectively\n",
    "\n",
    "##### For some new unknown words that were not in the vocaburaly, they will be assigned tokens of unknown and given token ids of unknown\n",
    "\n",
    "\n",
    "##### When working with multiple text sources we add <|endoftext|> tokens between the texts signaling end and start of different segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will add these to tokens to the list of all unique words that we created in the previous section\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### length of vocabulary has increased from 613 to 615"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For additional quick check, let's print last 5 entries of the updated vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('“leaving', 610)\n",
      "('…', 611)\n",
      "('›', 612)\n",
      "('<|endoftext|>', 613)\n",
      "('<|unk|>', 614)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's extend the simple tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([],.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most parasites  in tree planting,<|endoftext|>only evaluate themselves.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Most parasites  in tree planting,\"\n",
    "text2 = \"only evaluate themselves.\"\n",
    "\n",
    "text = \"<|endoftext|>\".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[130, 614, 383, 568, 484, 4, 614, 320, 558, 5]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
